{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix6Sc4hVIkS5"
      },
      "source": [
        "# Reinforcement Learning from AI Feedback (RLAIF)\n",
        "\n",
        "## Enhancing T5-Base Summarization with Proximal Policy Optimization (PPO) and PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee-7XQrPIkS-"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q trl\n",
        "!pip install -q peft\n",
        "!pip install -q numpy\n",
        "!pip install -q pandas\n",
        "!pip install -q tqdm\n",
        "!pip install -q openai\n",
        "!pip install -q wandb\n",
        "!pip install -U -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE2XRxq2IkTB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "\n",
        "from peft import PeftModel, PeftConfig,  TaskType\n",
        "\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftType,\n",
        "    LoraConfig,\n",
        ")\n",
        "\n",
        "# AutoModelForCausalLMWithValueHead & AutoModelForSeq2SeqLMWithValueHead: A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning.\n",
        "# https://huggingface.co/docs/trl/models#trl.AutoModelForSeq2SeqLMWithValueHead\n",
        "\n",
        "# trl: Transformer Reinforcement Learning library\n",
        "import trl\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead # https://huggingface.co/docs/trl/quickstart\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "# import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm library makes the loops show a smart progress meter.\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWvCQ2nIkTC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcYx683EOioM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "openai_api_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjNaIdoVa9nq"
      },
      "outputs": [],
      "source": [
        "orig_dataset = load_dataset('CarperAI/openai_summarize_comparisons', split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zTrDq6nbXih"
      },
      "outputs": [],
      "source": [
        "orig_dataset[10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5gETCZqOfKU"
      },
      "source": [
        "# RLHF Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnt__ajIkTF"
      },
      "outputs": [],
      "source": [
        "policy_model_path = \"JuanKO/rlhf_base_model\"\n",
        "policy_model_name = \"t5-base\"\n",
        "\n",
        "policy_model = T5ForConditionalGeneration.from_pretrained(policy_model_path)\n",
        "policy_model.to(device)\n",
        "policy_tokenizer = T5Tokenizer.from_pretrained(policy_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtiqUbdPIkTH"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.10,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # T5\n",
        ")\n",
        "\n",
        "policy_peft_model = get_peft_model(policy_model, lora_config)\n",
        "policy_peft_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_SFOc-1IkTH"
      },
      "outputs": [],
      "source": [
        "policy_peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txdIgFSkIkTI"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/trl/quickstart\n",
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(policy_peft_model,\n",
        "                                                               torch_dtype=torch.bfloat16,\n",
        "                                                               is_trainable=True)\n",
        "\n",
        "ppo_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl2ukCSBIkTI"
      },
      "outputs": [],
      "source": [
        "ref_model = create_reference_model(policy_model)\n",
        "ref_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hioROxkQIkTJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "orig_dataset = load_dataset('CarperAI/openai_summarize_comparisons', split='test')\n",
        "\n",
        "# Filter samples where the prompt length is less than or equal to 750\n",
        "filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt'].split()) <= 450) # By word\n",
        "#filtered_dataset = orig_dataset.filter(lambda example: len(example['prompt']) <= 1250) # By character\n",
        "\n",
        "# Shuffle and select the first 10K samples\n",
        "#shuffled_dataset = orig_dataset.shuffle(seed=42).select(range(1000))\n",
        "shuffled_dataset = filtered_dataset.shuffle(seed=42).select(range(2000))\n",
        "\n",
        "\n",
        "# Extract the desired features.  Renaming chose to response to follow the ppo library requirements.\n",
        "new_dataset_dict = {\n",
        "    \"prompt\": shuffled_dataset[\"prompt\"],\n",
        "    \"response\": shuffled_dataset[\"chosen\"]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a new Dataset\n",
        "dataset = HFDataset.from_dict(new_dataset_dict)\n",
        "\n",
        "# Split the new_dataset into train_dataset and eval_dataset\n",
        "split_ratio = 0.8  # 80% for training, 20% for evaluation\n",
        "num_train_samples = int(split_ratio * len(dataset))\n",
        "train_dataset = dataset.select(range(num_train_samples))\n",
        "eval_dataset = dataset.select(range(num_train_samples, len(dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-EWoFfIkTJ"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0].keys())\n",
        "print(eval_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5xBh6z9IkTP"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Instantiate your tokenizer (replace T5Tokenizer with your model's tokenizer if different)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\") # or whatever model you're using\n",
        "\n",
        "def tokenize_function(example):\n",
        "    # Tokenize the prompt and store it as input_ids. Also return the response.\n",
        "    return {\n",
        "        \"input_ids\": tokenizer(example[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=1024)[\"input_ids\"].squeeze(),\n",
        "        \"response\": example[\"response\"],\n",
        "    }\n",
        "\n",
        "# Tokenize the training and evaluation datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTx08dafIkTQ"
      },
      "outputs": [],
      "source": [
        "# Lets check one sample of the train_dataset\n",
        "print(train_dataset[0])  # print the first example from the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBf3f5PkIkTQ"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}, {\"key1\": \"value4\", \"key2\": \"value5\", \"key3\": \"value6\"}]\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')\n",
        "\n",
        "# Lets sample what the collator generates:\n",
        "sample_data = [train_dataset[i] for i in range(3)]  # take first three examples\n",
        "collated_data = collator(sample_data)\n",
        "print(collated_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJxRBHLl7L9R"
      },
      "outputs": [],
      "source": [
        "learning_rate=1e-4\n",
        "max_ppo_epochs=5\n",
        "mini_batch_size=2\n",
        "batch_size=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTi-YYjlIkTR"
      },
      "outputs": [],
      "source": [
        "# Check out https://huggingface.co/docs/trl/trainer\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=policy_model_name,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfL_CqlTIkTR"
      },
      "outputs": [],
      "source": [
        "# Check out https://huggingface.co/docs/trl/trainer\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=policy_tokenizer,\n",
        "                         dataset=train_dataset,\n",
        "                         data_collator=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VTGWfXg7ekw"
      },
      "outputs": [],
      "source": [
        "# Some initial values\n",
        "output_min_length = 128\n",
        "output_max_length = 2048\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# These hyperparams guide the generation of the completion in the policy model. We could add other params like temperature.\n",
        "generation_kwargs = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "max_ppo_steps = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkZ3fJM8CPnW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "def score_summaries(full_text, summarized_text):\n",
        "\n",
        "  prompt = f\"\"\"### FULL TEXT:\\n {full_text} \\n\n",
        "  ### SUMMARIZED TEXT: \\n {summarized_text}\"\"\"\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      temperature = 0.,\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[{\"role\": \"system\", \"content\": f\"\"\"You are an expert in text summarization. Below, you are given the full text and its summarization.\n",
        "  Your role is to rate the provided summarization with scores ranging from 0 to 1, where: 0 is the lowest score, 1 is the highest score.\n",
        "  Your response should only be a double precision number that represents the scoring rate.\n",
        "  \"\"\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}],\n",
        "      request_timeout=60000\n",
        "  )\n",
        "\n",
        "  response = response['choices'][0]['message']['content']\n",
        "  score    = float(re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", response)[0])\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KrKa3f7xNZL"
      },
      "outputs": [],
      "source": [
        "orig_dataset[10000]['prompt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOFkiLmUIkTS"
      },
      "outputs": [],
      "source": [
        "objective_kl    = []\n",
        "returns_mean    = []\n",
        "advantages_mean = []\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for step, batch in enumerate(ppo_trainer.dataloader):\n",
        "\n",
        "    if step >= max_ppo_steps: # Break when we reach max_steps.\n",
        "        break\n",
        "\n",
        "\n",
        "    prompts = [policy_tokenizer.decode(tok) for tok in batch['input_ids']][0]\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "    # print(batch['response'])\n",
        "    # if step==0: break\n",
        "\n",
        "    if isinstance(prompt_tensors, list) and all(isinstance(item, list) for item in prompt_tensors): # HACK!!! Check if original_prompt_tensors is a list of lists\n",
        "        lengths = [len(seq) for seq in prompt_tensors] # Verify if sequences have fixed or variable length\n",
        "        unique_lengths = set(lengths)\n",
        "\n",
        "        if len(unique_lengths) > 1: # If sequences have variable lengths, pad them\n",
        "            max_length = max(unique_lengths)\n",
        "            original_prompt_tensors = [seq + [0] * (max_length - len(seq)) for seq in prompt_tensors]  # padding with zeros\n",
        "\n",
        "        prompt_tensors = [torch.tensor(seq).to(device) for seq in prompt_tensors] # Convert original_prompt_tensors to individual tensors\n",
        "\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        prompt_tensor = torch.tensor(prompt_tensor).to(device)\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    batch[\"response\"] = [policy_tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    response = batch[\"response\"]\n",
        "\n",
        "    reward_tensors = []\n",
        "\n",
        "    for prompt, summary in zip(prompts, response):\n",
        "        score = score_summaries(prompt, response)\n",
        "        # score = float(score)\n",
        "        reward_tensors.append(torch.tensor(score))\n",
        "\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}') # Measures how different the policy's action distribution after the update is from the action distribution before the update. PPO tries to make these changes very small to avoid sudden changes.\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}') # This is the average return achieved by the agent. Higher is better.\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}') # Measures how much better an action is than the average action at a given state.\n",
        "    print(f'STEP: {step}')\n",
        "\n",
        "    objective_kl.append(stats[\"objective/kl\"])\n",
        "    returns_mean.append(stats[\"ppo/returns/mean\"])\n",
        "    advantages_mean.append(stats[\"ppo/policy/advantages_mean\"])\n",
        "\n",
        "    print('-'.join('' for x in range(100)))\n",
        "\n",
        "end = time.time()\n",
        "print(f'TIME: {end - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpFm4nQmtOCb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "t = np.array(returns_mean)\n",
        "s = range(len(returns_mean))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(s, t)\n",
        "\n",
        "ax.set(xlabel='episodes', ylabel='mean return',\n",
        "       title='Policy optimization')\n",
        "# ax.grid()\n",
        "\n",
        "fig.savefig(\"test.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yw12zuaIkTS"
      },
      "source": [
        "## Saving the Model and Tokenizer\n",
        "\n",
        "After the fine-tuning process, it's crucial to save the model's weights and the tokenizer's configuration for future use, whether it's for inference, further training, or sharing with the community.\n",
        "\n",
        "### 1. Saving the Model\n",
        "\n",
        "To preserve the state of your model post-training, use the `save_pretrained` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6H_cP0eeaWV"
      },
      "outputs": [],
      "source": [
        "ppo_trainer.model.push_to_hub('PanoEvJ/T5_summarization_RLAIF', token='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')\n",
        "policy_tokenizer.push_to_hub('PanoEvJ/T5_summarization_RLAIF', token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ZO9z-_IiGb"
      },
      "outputs": [],
      "source": [
        "objective_kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5U5GDmwIvDk"
      },
      "outputs": [],
      "source": [
        "returns_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyEL-Y3EIzxv"
      },
      "outputs": [],
      "source": [
        "advantages_mean"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
